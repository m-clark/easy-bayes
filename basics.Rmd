# (PART\*) Part I: Getting Started {-}

# Basic Bayesian Analysis

The classic formula ($\theta$ = parameters):

$$p(\theta|\mathcal{Data}) = \frac{p(\mathcal{Data}|\theta)p(\theta)}{p(\mathcal{Data})}$$


Conceptually, we can think about it in different ways

$$\mathrm{posterior} \propto likelihood * prior$$

Standard methods you are already familiar with begin and end with likelihood

They can be seen as Bayesian analysis with uninformative priors

Alternatively, we can think also in terms of the posterior as the combination hypothesis, and evidence, in the form of data.<br>

$$p(hypothesis|data) \propto p(data|hypothesis)p(hypothesis)$$

$$\text{updated belief} = \text{current evidence} * \text{prior belief or evidence}$$
<br>
Some key distinctions: 

- Focus on distributions and uncertainty estimation instead of point estimates
- More natural interpretation of results
- Easy model criticism

http://micl.shinyapps.io/prior2post/

<br>
<img src="img/bayes_doc/modified_bayes_theorem.png" style="display:block; margin: 0 auto;" width=50%>

# Advantages

- Incorporation of prior information
- Many ways to explore the model easily
- Tools that can handle complex models without having to change the general approach
- Ability to handle small samples with appropriate guard against overfitting
- A more intuitive inferential framework
- You'll never have so much fun finding out why your model sucks!


